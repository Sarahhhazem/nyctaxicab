# -*- coding: utf-8 -*-
"""TaxiRide Hotspot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YB1KfpJeX-IFJloq-PJ75_28ZIPE4j8R

# Data Collection and Cleaning
"""

import os
os.getcwd()

path = "/Users/peteliur/Desktop/BA Project/"

weather_link = "Climate_Data_NCEI.csv"
weather_data = path + weather_link

crash_link = "Motor_Vehicle_Collisions_-_Crashes.csv"
crash_data = path + crash_link

taxi_link = "2018_Yellow_Taxi_Trip_Data.csv"
taxi_data = path + taxi_link

taxizone_link = "taxi_zone_lookup.csv"
taxizone_data = path + taxizone_link

import pandas as pd
import numpy as np

"""This is cleaning the weather data"""

weather = pd.read_csv(weather_data,sep=",",error_bad_lines=False)

weather = weather[weather['STATION'] == 'USW00094728']

weather = weather.reset_index()

#weather data is for NYC
#AWND is Average Daily Wind Speed
#PRCP is Precipitation
#TMAX is max temperature; TMIN is min temperature
weather = weather[['DATE','AWND','PRCP','SNOW','TMAX','TMIN']]

#TAVG is the average temperature for the day
weather['TAVG'] = weather[['TMAX', 'TMIN']].mean(axis=1)

weather = weather[['DATE','AWND','PRCP','SNOW','TAVG']]

weather = weather.dropna(subset=['DATE'])

weather['DATE'] = pd.to_datetime(weather['DATE'],format='%Y-%m-%d')

"""the weather data is data for every day in NYC; it gives the date, average wind speed, precipitation amount, amount of snow,
and the average temperature that day

This is to clean the crash data
"""

crash = pd.DataFrame()
run = 0
chunk_size = 100000
intermediate_data = []
for count, chunk in enumerate(pd.read_csv(crash_data, sep=",", error_bad_lines=False, chunksize=chunk_size)):
    #print('Chunk: ' + str(count + 1))
    relevant_rows = []
    for i in range(0 + run, len(chunk) + run):
        if '2018' in chunk['CRASH DATE'][i]:
            relevant_rows.append(i-run)
    #print('Number of rows selected: ' + str(len(relevant_rows)) + ' / ' + str(chunk_size))
    intermediate_data.append(chunk.iloc[relevant_rows])
    run = run + chunk_size

crash = crash.append(intermediate_data)

crash = crash.reset_index()

crash = crash[['CRASH DATE','CRASH TIME','BOROUGH','NUMBER OF PERSONS INJURED','NUMBER OF PERSONS KILLED',
               'CONTRIBUTING FACTOR VEHICLE 1','VEHICLE TYPE CODE 1']]

crash = crash.dropna(subset=['BOROUGH'])

crash['CRASH DATE'] = pd.to_datetime(crash['CRASH DATE'],format='%m/%d/%Y')

"""The crash data gives all motor vehicle crashes in NYC by date, time, and borough
it includes the number of people injured, number of people killed, the contributing factor to the crash, and the vehicle type

Cleaning the taxi zone data
"""

taxizone = pd.read_csv(taxizone_data,sep=",",error_bad_lines=False)



taxizone = taxizone[['LocationID','borough', 'the_geom']]

"""The taxi zone data gives the LocationID and the Borough for each Location ID
this is for the taxi data, as it only gives the PULocationID and DOLocationID, so this data set will clarify those locations

Loading and cleaning the taxi data
"""

import time
start_time = time.time()
taxi = pd.DataFrame()
run = 0
chunk_size = 100000
intermediate_data = []
for count, chunk in enumerate(pd.read_csv(taxi_data, sep=",", error_bad_lines=False, chunksize=chunk_size)):
    print('Chunk: ' + str(count + 1))
    relevant_rows = []
    for i in range(0 + run, len(chunk) + run,5):
        if '2018' in chunk['tpep_pickup_datetime'][i]:
            relevant_rows.append(i-run)
    print('Number of rows selected: ' + str(len(relevant_rows)) + ' / ' + str(chunk_size))
    intermediate_data.append(chunk.iloc[relevant_rows])
    run = run + chunk_size

taxi = taxi.append(intermediate_data)
elapsed_time = time.time() - start_time
print(elapsed_time)
#takes about 14 mins to run

taxi = taxi.reset_index()

taxi = taxi[['VendorID','tpep_pickup_datetime','tpep_dropoff_datetime','passenger_count','trip_distance','PULocationID',
            'DOLocationID','fare_amount','tip_amount','total_amount']]

taxi = taxi.dropna(subset=['tpep_pickup_datetime','tpep_dropoff_datetime','trip_distance','PULocationID','DOLocationID',
                           'fare_amount','tip_amount','total_amount'])

#this function takes about 5 minutes to run
taxi['tpep_pickup_datetime'] = pd.to_datetime(taxi['tpep_pickup_datetime'],format='%m/%d/%Y %I:%M:%S %p')
taxi['tpep_dropoff_datetime'] = pd.to_datetime(taxi['tpep_dropoff_datetime'],format='%m/%d/%Y %I:%M:%S %p')

taxi['ride_time'] =  taxi['tpep_dropoff_datetime'] - taxi['tpep_pickup_datetime']

"""The taxi data gives information on every Yellow Taxi in NYC by date and time 
the information includes passenger count, trip distance, pickup location and dropoff location, fare amount, tip amount, and the total amount paid (after taxes and other fees)

# Part 1: Regression on Tips

First we have to add the weather data to the taxi data
"""

#have to use cut off some of the data because getting memory error further down
lm_taxi = taxi[::50]

lm_taxi['DATE'] = lm_taxi['tpep_pickup_datetime'].dt.date

lm_taxi['DATE'] = pd.to_datetime(lm_taxi['DATE'],format='%Y-%m-%d')

lm_taxi_weather = lm_taxi.merge(weather, on='DATE',how='left')

lm_taxi_weather.to_csv("/Users/sonat/Desktop/Business Analytics/Group Project/lm_taxi_weather.csv")

#DONT RUN THIS
#IT'S NOT WORKING

import statsmodels.api as sm

X = lm_taxi_weather[['tpep_pickup_datetime','passenger_count','tpep_dropoff_datetime','trip_distance','PULocationID',
                    'DOLocationID','fare_amount','ride_time','AWND','PRCP','SNOW','TAVG']]
Y = lm_taxi_weather['tip_amount']

X = sm.add_constant(X) 
model = sm.OLS(Y, X).fit() 
model.summary()



"""# Hotspot Visual"""

#import geojson
import json
f = open('nyu-2451-36743-geojson.json')
zones_geojson= json.load(f)



#count the number of pick ups and drop offs within for each location
pickup = pd.DataFrame(taxi.groupby('PULocationID').size(),)
pickup.rename(columns = {0: 'count'},inplace = True)
pickup = pickup.reset_index()
#pumakeup = pd.DataFrame([[103,1],[104,1],[110,1]], columns = ['PULocationID','count'])
#pickup = pickup.append(pumakeup)
pickup['logcount'] = np.log(pickup['count'])



dropoff = pd.DataFrame(taxi.groupby('DOLocationID').size())
dropoff.rename(columns = {0: 'count'},inplace = True)
dropoff['logcount'] = np.log(dropoff['count'])
dropoff = dropoff.reset_index()



#make sure geojson and pickup contain the same location id
geojson_id = []
for i in zones_geojson['features']:
        geojson_id.append(i['properties']['locationid'])

pickup = pickup[pickup['PULocationID'].isin(geojson_id)]
pickup

zones_geojson

feature = []

pickup_id = list(pickup['PULocationID'].unique())
pickup_id

for i in zones_geojson['features']:
    if i['properties']['locationid'] in pickup_id and i['properties']['locationid'] != 56 :
        feature.append(i)
    

for i in zones_geojson['features']:
    if i['properties']['locationid'] == 56:
        feature.append(i)
        break

len(feature)
new_geojson = {}
new_geojson['type'] = 'FeatureCollection'
new_geojson['totalFeatures'] = 258
new_geojson['features'] = feature
new_geojson['crs']= {'type': 'name', 'properties': {'name': 'urn:ogc:def:crs:EPSG::4326'}}
new_geojson['bbox']= [-74.2583694458008,40.4940147399902,-73.6972274780273,40.9176330566406]

new_geojson['features'][106]['properties']['locationid']



#find taxi zones that can actually be located
matchableID = []
for i in taxizone['LocationID'].dropna().unique():
    if len(i)<=3:
        matchableID.append(int(i))
print(matchableID)

taxizone = taxizone[taxizone['PULocation']]





!pip install --upgrade pip folium

"""# Popular pickup spots"""

import folium
m = folium.Map( location = [40.7589,-73.9851], zoom_start = 10)
folium.features.GeoJson(zones_geojson).add_to(m)
c = folium.Choropleth(geo_data = zones_geojson,
                      data = pickup, 
                     columns = ['PULocationID', 'logcount'],
                     key_on = 'feature.properties.locationid',
                     fill_color = 'PRGn',
                     legend_name = 'Log counts',
                     highlight = True)
c.add_to(m)


m

"""# Popular Dropoff Spots"""

n = folium.Map( location = [40.7589,-73.9851], zoom_start = 10)
folium.features.GeoJson(zones_geojson).add_to(n)
c = folium.Choropleth(geo_data = zones_geojson,
                      data = dropoff, 
                     columns = ['DOLocationID', 'logcount'],
                     key_on = 'feature.properties.locationid',
                     fill_color = 'PRGn',
                     legend_name = 'Log counts',
                     highlight = True)
c.add_to(n)


n

"""# Destination with 15% tips"""

taxi['tip/fare'] = taxi['tip_amount']/taxi['fare_amount']
taxi[taxi['tip/fare'] >= 0.15]

len(taxi[taxi['tip/fare'] >= 0.12])/len(taxi)

sometip = pd.DataFrame(taxi[taxi['tip/fare'] >= 0.12].groupby('DOLocationID').size())
sometip.rename(columns = {0: 'count'},inplace = True)
sometip['logcount'] = np.log(sometip['count'])
sometip = sometip.reset_index()

t = folium.Map( location = [40.7589,-73.9851], zoom_start = 10)
folium.features.GeoJson(zones_geojson).add_to(t)
c = folium.Choropleth(geo_data = zones_geojson,
                      data = sometip, 
                     columns = ['DOLocationID', 'logcount'],
                     key_on = 'feature.properties.locationid',
                     fill_color = "RdYlGn",
                     legend_name = 'Log counts',
                     highlight = True)
c.add_to(t)


t

"""# PickUp Locations with 15% tips"""

starttip = pd.DataFrame(taxi[taxi['tip/fare'] >= 0.15].groupby('PULocationID').size())
starttip.rename(columns = {0: 'count'},inplace = True)
starttip['logcount'] = np.log(starttip['count'])
starttip = starttip.reset_index()

t2 = folium.Map( location = [40.7589,-73.9851], zoom_start = 10)
folium.features.GeoJson(zones_geojson).add_to(t2)
c = folium.Choropleth(geo_data = zones_geojson,
                      data = starttip, 
                     columns = ['PULocationID', 'logcount'],
                     key_on = 'feature.properties.locationid',
                     fill_color = "RdYlGn",
                     legend_name = 'Log counts',
                     highlight = True)
c.add_to(t2)


t2

"""# Pickup spots with long distance travel"""

longdistance = pd.DataFrame(taxi[taxi['trip_distance'] > 3].groupby('PULocationID').size())
longdistance.rename(columns = {0: 'count'},inplace = True)
longdistance['logcount'] = np.log(longdistance['count'])
longdistance = longdistance.reset_index()

long = folium.Map( location = [40.7589,-73.9851], zoom_start = 10)
folium.features.GeoJson(zones_geojson).add_to(long)
c = folium.Choropleth(geo_data = zones_geojson,
                      data = longdistance, 
                     columns = ['PULocationID', 'logcount'],
                     key_on = 'feature.properties.locationid',
                     fill_color = "BuGn",
                     legend_name = 'Log counts',
                     highlight = True)
c.add_to(long)


long

"""# Popular Pickup spots in the morning"""

time= taxi.set_index('tpep_pickup_datetime').between_time('06:30:00','10:00:00')
time

morning = pd.DataFrame(time.groupby('PULocationID').size())
morning.rename(columns = {0: 'count'},inplace = True)
morning['logcount'] = np.log(morning['count'])
morning = morning.reset_index()

morning_rides = folium.Map( location = [40.7589,-73.9851], zoom_start = 10)
folium.features.GeoJson(zones_geojson).add_to(morning_rides)
c = folium.Choropleth(geo_data = zones_geojson,
                      data = morning, 
                     columns = ['PULocationID', 'logcount'],
                     key_on = 'feature.properties.locationid',
                     fill_color = "RdYlGn",
                     legend_name = 'Log counts',
                     highlight = True)
c.add_to(morning_rides)


morning_rides

"""# Popular Pickup Spots 5pm-9pm"""

evening_df= taxi.set_index('tpep_pickup_datetime').between_time('17:30:00','21:00:00')

evening = pd.DataFrame(evening_df.groupby('PULocationID').size())
evening.rename(columns = {0: 'count'},inplace = True)
evening['logcount'] = np.log(evening['count'])
evening = evening.reset_index()

evening_rides = folium.Map( location = [40.7589,-73.9851], zoom_start = 10)
folium.features.GeoJson(zones_geojson).add_to(evening_rides)
c = folium.Choropleth(geo_data = zones_geojson,
                      data = evening, 
                     columns = ['PULocationID', 'logcount'],
                     key_on = 'feature.properties.locationid',
                     fill_color = "RdYlGn",
                     legend_name = 'Log counts',
                     highlight = True)
c.add_to(evening_rides)


evening_rides

"""# Popular pickup spots 10pm - 3 am"""

night_df= taxi.set_index('tpep_pickup_datetime').between_time('22:00:00','3:00:00')

night = pd.DataFrame(night_df.groupby('PULocationID').size())
night.rename(columns = {0: 'count'},inplace = True)
night['logcount'] = np.log(night['count'])
night = night.reset_index()

night_rides = folium.Map( location = [40.7589,-73.9851], zoom_start = 10)
folium.features.GeoJson(zones_geojson).add_to(evening_rides)
c = folium.Choropleth(geo_data = zones_geojson,
                      data = night, 
                     columns = ['PULocationID', 'count'],
                     key_on = 'feature.properties.locationid',
                     fill_color = "RdYlGn",
                     legend_name = 'Log counts',
                     highlight = True)
c.add_to(night_rides)


night_rides

